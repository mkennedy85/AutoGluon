# AutoGluon Tabular (Multimodal)

This repository contains a **reproducible AutoML demonstration** built with [AutoGluon Tabular](https://auto.gluon.ai/stable/index.html), showcasing a multimodal pipeline that combines **tabular, text, and image features** to predict pet adoption speed using the PetFinder dataset.

---

## ğŸ§© Project Overview

This notebook demonstrates the use of **AutoGluonâ€™s TabularPredictor** to automatically train, ensemble, and evaluate multiple models with minimal code, while maintaining reproducibility and structured experiment outputs.

It includes a self-contained workflow designed for **Google Vertex AI Workbench**, **JupyterLab**, or **Colab** environments.

### Highlights

- **Portable & reproducible:** All generated artifacts are saved under `./artifacts/` for isolation and clarity  
- **Multimodal support:** Integrates text, numeric, and image columns using AutoGluonâ€™s feature metadata system  
- **GPU-aware configuration:** Automatically skips the deep multimodal model if no GPU is available  
- **Explainable:** Includes summaries, visualizations, and reports to interpret model behavior  
- **Practical:** Exports leaderboards, predictions, and manifests for automation and grading

---

## ğŸ§  Workflow Summary

1. **Data Preparation**
   - Downloads and unpacks the **PetFinder Kaggle** dataset.
   - Creates a local folder structure under `./artifacts/`.
   - Samples 500 training rows for fast experimentation.

2. **Feature Engineering**
   - Parses text (pet descriptions) and image paths.
   - Declares feature types explicitly using AutoGluonâ€™s `FeatureMetadata`.

3. **Model Training**
   - Trains multiple models: `LightGBM`, `CatBoost`, `XGBoost`, and `NeuralNetTorch`.
   - Builds an ensemble (`WeightedEnsemble_L2`) that outperforms individual models.
   - Uses a 300-second time limit for reproducibility.

4. **Evaluation**
   - Saves outputs to `./artifacts/` including:
     - `leaderboard.csv` â€” model ranking and metrics
     - `predictions.csv` â€” predicted adoption speeds
     - `feature_importance.csv` â€” model interpretability (optional)
     - `manifest.json` â€” pointers to all generated outputs

5. **Visualization**
   - Displays model leaderboard and class prediction distribution.

6. **Interpretation**
   - The ensemble achieved the highest validation accuracy (~0.47).  
   - Prediction distribution reflects dataset imbalance, dominated by class 4 outcomes.

---

## ğŸ“Š Key Results

| Model | Validation Accuracy | Notes |
|-------|---------------------|-------|
| WeightedEnsemble_L2 | 0.47 | Combined strengths of top models |
| CatBoost | 0.44 | Strong single-model baseline |
| NeuralNetTorch | 0.40 | Deep model without GPU |
| LightGBMLarge | 0.39 | Larger feature space variant |
| XGBoost | 0.38 | Consistent, interpretable gradient boosting |

---

## ğŸ§­ Insights

- AutoGluonâ€™s **ensembling** substantially improves performance with minimal tuning.
- Dataset **class imbalance** affects precision/recall for minority classes.
- The modular structure allows extension to GPU multimodal training (`AG_AUTOMM`).

---

## âš™ï¸ Environment

| Component | Version |
|------------|----------|
| Python | 3.10 |
| AutoGluon | 1.4.0 |
| pandas | 2.x |
| scikit-learn | 1.3+ |

To reproduce this environment via `conda`:

```bash
conda create -n autogluon python=3.10 -y
conda activate autogluon
pip install autogluon~=1.4 pandas scikit-learn matplotlib
```

---

## â–¶ï¸ How to Run

1. Launch your Jupyter or Vertex AI Workbench instance.  
2. Open the notebook `tabular_multimodal.ipynb`.  
3. Execute all cells sequentially.  
4. Explore generated artifacts in the `artifacts/` directory.

To view your leaderboard:

```bash
cat artifacts/leaderboard.csv | head -n 10
```

---

## ğŸ§¾ Repository Structure

```
â”œâ”€â”€ tabular_multimodal.ipynb     # Main notebook
â”œâ”€â”€ artifacts/                   # Generated outputs
â”‚   â”œâ”€â”€ AutoGluonModels/         # Saved model folder
â”‚   â”œâ”€â”€ leaderboard.csv
â”‚   â”œâ”€â”€ predictions.csv
â”‚   â”œâ”€â”€ manifest.json
â”‚   â”œâ”€â”€ feature_importance.csv
â”‚   â””â”€â”€ run.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Visualization Example

Below is an example of the validation performance and class distribution generated at the end of the notebook:

![Leaderboard Visualization](assets/top_models.png)

![Prediction Distribution](assets/pred_distribution.png)

---

## ğŸ” Interpretation of Results

- **Top AutoGluon Models â€” Validation Performance:**  
  WeightedEnsemble_L2 achieved the best accuracy, showing the power of AutoGluonâ€™s ensembling.  
  CatBoost and XGBoost followed closely, indicating strong generalization.

- **Prediction Distribution:**  
  Class 4 dominates, followed by class 1 â€” consistent with the training data imbalance.

**Next Steps:**
- Enable GPU and reintroduce multimodal deep models (`AG_AUTOMM`).
- Perform hyperparameter tuning or longer training.
- Add cross-validation for more robust evaluation.

---

**Author:** Michael Kennedy  
**Date:** October 2025  
**License:** MIT
